{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Jm3UN_Hfsu"
      },
      "source": [
        "## **Homework 3**\n",
        "**Instructions**\n",
        "* This homework focuses on understanding and applying DETR for object detection and attention visualization. It consists of **three questions** designed to assess both theoretical understanding and practical application.\n",
        "\n",
        "* Please organize your answers and results for the questions below and submit this jupyter notebook as **a .pdf file**.\n",
        "\n",
        "* **Deadline: 11/14 (Thur) 23:59**\n",
        "\n",
        "**Reference**\n",
        "* End-to-End Object Detection with Transformers (DETR): https://github.com/facebookresearch/detr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3n9blo4JO7m"
      },
      "source": [
        "### **Q1.  Understanding DETR model**\n",
        "\n",
        "* Fill-in-the-blank exercise to test your understanding of critical parts of the DETR model workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SONlVIhPH_qF"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class DETR(nn.Module):\n",
        "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6, num_queries=100):\n",
        "        super().__init__()\n",
        "\n",
        "        # create ResNet-50 backbone\n",
        "        self.backbone = resnet50()\n",
        "        del self.backbone.fc\n",
        "\n",
        "        # create conversion layer\n",
        "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
        "\n",
        "        # create a default PyTorch transformer\n",
        "        self.transformer = nn.Transformer(\n",
        "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "        # prediction heads, one extra class for predicting non-empty slots\n",
        "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
        "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # output positional encodings (object queries)\n",
        "        self.query_pos = nn.Parameter(torch.rand(num_queries, hidden_dim))\n",
        "\n",
        "        # spatial positional encodings\n",
        "        # note that in baseline DETR we use sine positional encodings\n",
        "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
        "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
        "        x = self.backbone.conv1(inputs)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.backbone.layer4(x)\n",
        "\n",
        "        # convert from 2048 to 256 feature planes for the transformer\n",
        "        h = self.conv(x)\n",
        "\n",
        "        # construct positional encodings\n",
        "        H, W = h.shape[-2:]\n",
        "        pos = torch.cat([\n",
        "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
        "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
        "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
        "\n",
        "        # propagate through the transformer\n",
        "        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
        "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
        "\n",
        "\n",
        "\n",
        "        # finally project transformer outputs to class labels and bounding boxes\n",
        "        pred_logits = self.linear_class(h)\n",
        "        pred_boxes = self.linear_bbox(h).sigmoid()  # self.linear_bbox(h).sigmoid()?\n",
        "\n",
        "        return {'pred_logits': pred_logits,\n",
        "                'pred_boxes': pred_boxes}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGZlqo-HtRN"
      },
      "source": [
        "### **Q2. Custom Image Detection and Attention Visualization**\n",
        "\n",
        "In this task, you will upload an **image of your choice** (different from the provided sample) and follow the steps below:\n",
        "\n",
        "* Object Detection using DETR\n",
        " * Use the DETR model to detect objects in your uploaded image.\n",
        "\n",
        "* Attention Visualization in Encoder\n",
        " * Visualize the regions of the image where the encoder focuses the most.\n",
        "\n",
        "* Decoder Query Attention in Decoder\n",
        " * Visualize how the decoder’s query attends to specific areas corresponding to the detected objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExsO0XgWHsvP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "from torchvision.models import resnet50\n",
        "import torchvision.transforms as T\n",
        "torch.set_grad_enabled(False);\n",
        "\n",
        "# COCO classes\n",
        "CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "# standard PyTorch mean-std input image normalization\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def plot_results(pil_img, prob, boxes):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=3))\n",
        "        cl = p.argmax()\n",
        "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
        "        ax.text(xmin, ymin, text, fontsize=15,\n",
        "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TSPybYnIwSE"
      },
      "source": [
        "\n",
        "In this section, we show-case how to load a model from hub, run it on a custom image, and print the result.\n",
        "Here we load the simplest model (DETR-R50) for fast inference. You can swap it with any other model from the model zoo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aX2QNanH9T0"
      },
      "outputs": [],
      "source": [
        "model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "model.eval();\n",
        "\n",
        "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "url = 'https://farm1.staticflickr.com/51/129657300_a033ce9282_z.jpg'\n",
        "im = Image.open(requests.get(url, stream=True).raw) # put your own image\n",
        "\n",
        "# mean-std normalize the input image (batch-size: 1)\n",
        "img = transform(im).unsqueeze(0)\n",
        "\n",
        "# propagate through the model\n",
        "outputs = model(img)\n",
        "\n",
        "# keep only predictions with 0.7+ confidence\n",
        "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "keep = probas.max(-1).values > 0.9\n",
        "\n",
        "# convert boxes from [0; 1] to image scales\n",
        "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "\n",
        "# mean-std normalize the input image (batch-size: 1)\n",
        "img = transform(im).unsqueeze(0)\n",
        "\n",
        "# propagate through the model\n",
        "outputs = model(img)\n",
        "\n",
        "# keep only predictions with 0.7+ confidence\n",
        "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "keep = probas.max(-1).values > 0.9\n",
        "\n",
        "# convert boxes from [0; 1] to image scales\n",
        "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "\n",
        "# mean-std normalize the input image (batch-size: 1)\n",
        "img = transform(im).unsqueeze(0)\n",
        "\n",
        "# propagate through the model\n",
        "outputs = model(img)\n",
        "\n",
        "# keep only predictions with 0.7+ confidence\n",
        "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "keep = probas.max(-1).values > 0.9\n",
        "\n",
        "# convert boxes from [0; 1] to image scales\n",
        "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "\n",
        "plot_results(im, probas[keep], bboxes_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4alpH62I5GS"
      },
      "source": [
        "\n",
        "Here we visualize attention weights of the last decoder layer. This corresponds to visualizing, for each detected objects, which part of the image the model was looking at to predict this specific bounding box and class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCNmOSjGH9WV"
      },
      "outputs": [],
      "source": [
        "# use lists to store the outputs via up-values\n",
        "conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
        "\n",
        "hooks = [\n",
        "    model.backbone[-2].register_forward_hook(\n",
        "        lambda self, input, output: conv_features.append(output)\n",
        "    ),\n",
        "    model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
        "        lambda self, input, output: enc_attn_weights.append(output[1])\n",
        "    ),\n",
        "    model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
        "        lambda self, input, output: dec_attn_weights.append(output[1])\n",
        "    ),\n",
        "]\n",
        "\n",
        "# propagate through the model\n",
        "outputs = model(img) # put your own image\n",
        "\n",
        "for hook in hooks:\n",
        "    hook.remove()\n",
        "\n",
        "# don't need the list anymore\n",
        "conv_features = conv_features[0]\n",
        "enc_attn_weights = enc_attn_weights[0]\n",
        "dec_attn_weights = dec_attn_weights[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrrWPnotI_G-"
      },
      "outputs": [],
      "source": [
        "# get the feature map shape\n",
        "h, w = conv_features['0'].tensors.shape[-2:]\n",
        "\n",
        "fig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=2, figsize=(22, 7))\n",
        "colors = COLORS * 100\n",
        "for idx, ax_i, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), axs.T, bboxes_scaled):\n",
        "    ax = ax_i[0]\n",
        "    ax.imshow(dec_attn_weights[0, idx].view(h, w))\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'query id: {idx.item()}')\n",
        "    ax = ax_i[1]\n",
        "    ax.imshow(im)\n",
        "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                               fill=False, color='blue', linewidth=3))\n",
        "    ax.axis('off')\n",
        "    ax.set_title(CLASSES[probas[idx].argmax()])\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBaCqpWcJBRz"
      },
      "outputs": [],
      "source": [
        "# output of the CNN\n",
        "f_map = conv_features['0']\n",
        "print(\"Encoder attention:      \", enc_attn_weights[0].shape)\n",
        "print(\"Feature map:            \", f_map.tensors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th7dtFsDHsyh"
      },
      "outputs": [],
      "source": [
        "# get the HxW shape of the feature maps of the CNN\n",
        "shape = f_map.tensors.shape[-2:]\n",
        "# and reshape the self-attention to a more interpretable shape\n",
        "sattn = enc_attn_weights[0].reshape(shape + shape)\n",
        "print(\"Reshaped self-attention:\", sattn.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eRkeRezH8fX"
      },
      "outputs": [],
      "source": [
        "# downsampling factor for the CNN, is 32 for DETR and 16 for DETR DC5\n",
        "fact = 32\n",
        "\n",
        "# let's select 4 reference points for visualization\n",
        "idxs = [(200, 200), (280, 400), (200, 600), (440, 800),]\n",
        "\n",
        "# here we create the canvas\n",
        "fig = plt.figure(constrained_layout=True, figsize=(25 * 0.7, 8.5 * 0.7))\n",
        "# and we add one plot per reference point\n",
        "gs = fig.add_gridspec(2, 4)\n",
        "axs = [\n",
        "    fig.add_subplot(gs[0, 0]),\n",
        "    fig.add_subplot(gs[1, 0]),\n",
        "    fig.add_subplot(gs[0, -1]),\n",
        "    fig.add_subplot(gs[1, -1]),\n",
        "]\n",
        "\n",
        "# for each one of the reference points, let's plot the self-attention\n",
        "# for that point\n",
        "for idx_o, ax in zip(idxs, axs):\n",
        "    idx = (idx_o[0] // fact, idx_o[1] // fact)\n",
        "    ax.imshow(sattn[..., idx[0], idx[1]], cmap='cividis', interpolation='nearest')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'self-attention{idx_o}')\n",
        "\n",
        "# and now let's add the central image, with the reference points as red circles\n",
        "fcenter_ax = fig.add_subplot(gs[:, 1:-1])\n",
        "fcenter_ax.imshow(im)\n",
        "for (y, x) in idxs:\n",
        "    scale = im.height / img.shape[-2]\n",
        "    x = ((x // fact) + 0.5) * fact\n",
        "    y = ((y // fact) + 0.5) * fact\n",
        "    fcenter_ax.add_patch(plt.Circle((x * scale, y * scale), fact // 2, color='r'))\n",
        "    fcenter_ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bcx4iX5H1od"
      },
      "source": [
        "### **Q3. Understanding Attention Mechanisms**\n",
        "\n",
        "In this task, you focus on understanding the attention mechanisms present in the encoder and decoder of DETR.\n",
        "\n",
        "* Briefly describe the types of attention used in the encoder and decoder, and explain the key differences between them.\n",
        "\n",
        "* => DETR의 인코더에서는 self-attention이 사용된다. 입력 이미지의 각 부분들(patch)이 이미지의 다른 patch들에 대해 attend하여 이미지의 공간적인 정보/관계를 잘 습득하게 되며 인코딩을 진행하게 된다. 디코더에서는 self-attention과 cross-attention이 함께 사용된다. 우선 self-attention에서는 디코더의 입력값인 object queries에 대해서 attention을 적용한다. 디코더의 값만을 가지고 연산을 진행하는 것이다. 반면 cross-attention은 이러한 self-attention과 차이가 있는데, 가장 근본적인 차이점은 cross-attention에서는 인코더에서의 출력값과 디코더의 값을 함께 사용한다는 것이다. 디코더의 cross-attention에서는 디코더의 값인 object queries와 인코더의 값인 image features를 함께 사용하여 attention을 연산하게 된다.\n",
        "\n",
        "* Based on the visualized results from Q2, provide an analysis of the distinct characteristics of each attention mechanism in the encoder and decoder. Feel free to express your insights.\n",
        "\n",
        "* => 마지막 디코더 레이어의 attention weights를 시각화한 이미지에서는, 모델이 어떠한 오브젝트의 bounding box와 class를 예측하기 위해 이미지의 어느 부분에 많이 attend하였는지를 보여준다. 이는 해당 오브젝트가 위치하는 부분과 일치하였는데, 디코더에서 cross-attention을 진행할 때 object queries와 image feautures를 함께 사용해 attention을 진행하면서, 오브젝트가 이미지의 어디에 위치하는지 그 위치적인 정보를 잘 알아낸다는 것을 알 수 있다. 반면 인코더에서 self-attention을 진행할 때는, 주어진 입력 이미지의 전체적인 공간적 정보를 이해한다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}